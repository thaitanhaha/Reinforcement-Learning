{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom datasets import load_dataset, Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:07.890421Z","iopub.execute_input":"2025-02-13T16:46:07.890826Z","iopub.status.idle":"2025-02-13T16:46:34.633271Z","shell.execute_reply.started":"2025-02-13T16:46:07.890759Z","shell.execute_reply":"2025-02-13T16:46:34.632308Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model_name = \"google/flan-t5-large\"\nt5_model = T5ForConditionalGeneration.from_pretrained(model_name)\nt5_tokenizer = T5Tokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:34.634187Z","iopub.execute_input":"2025-02-13T16:46:34.634727Z","iopub.status.idle":"2025-02-13T16:46:54.410294Z","shell.execute_reply.started":"2025-02-13T16:46:34.634702Z","shell.execute_reply":"2025-02-13T16:46:54.409158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4370af7b2444e32b27c0de421b58f65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7443f7f96c6c4a77add79222e0e194b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f871856180e6405c9a2fc955c8337d7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1ada0b12784c8fbd268feb71ccb01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5ff38b9a624a6189a7bb617f03ca64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c769ff1eb52b4b4ba7cd54c61eddfef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0b25cba12914d629d6ed7f1c5a4a123"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"custom_dataset = [\n    {\"input\": \"Rewrite in a formal tone: Gotta get this done ASAP!\"},\n    {\"input\": \"Rewrite in a formal tone: U coming to the party?\"},\n    {\"input\": \"Rewrite in a formal tone: This place is lit!\"},\n    {\"input\": \"Rewrite in a formal tone: Can’t believe this dude won!\"},\n    {\"input\": \"Rewrite in a formal tone: Lemme know when u free.\"},\n    {\"input\": \"Rewrite in a formal tone: That’s my bad.\"},\n    {\"input\": \"Rewrite in a formal tone: Yo, what’s up?\"},\n    {\"input\": \"Rewrite in a formal tone: Gimme a sec.\"},\n    {\"input\": \"Rewrite in a formal tone: I ain’t got time for this.\"},\n    {\"input\": \"Rewrite in a formal tone: U gotta check this out!\"},\n]\n\ndataset = Dataset.from_list(custom_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:54.411606Z","iopub.execute_input":"2025-02-13T16:46:54.411954Z","iopub.status.idle":"2025-02-13T16:46:54.426259Z","shell.execute_reply.started":"2025-02-13T16:46:54.411926Z","shell.execute_reply":"2025-02-13T16:46:54.425140Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def test_model(input_text):\n    formatted_input = f\"Rewrite in a formal tone: {input_text}\"\n    inputs = t5_tokenizer(formatted_input, return_tensors=\"pt\", padding=True, truncation=True)\n    outputs = t5_model.generate(**inputs, max_length=50)\n    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ntest_sentences = [\n    \"Wanna grab a coffee?\",\n    \"Hey dude, what’s up?\",\n    \"No way that actually happened!\",\n    \"Gimme a min to check.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:54.428886Z","iopub.execute_input":"2025-02-13T16:46:54.429250Z","iopub.status.idle":"2025-02-13T16:46:55.731375Z","shell.execute_reply.started":"2025-02-13T16:46:54.429220Z","shell.execute_reply":"2025-02-13T16:46:55.729942Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Before train","metadata":{}},{"cell_type":"code","source":"for sent in test_sentences:\n    print(f\"Informal: {sent}\")\n    print(f\"Formal: {test_model(sent)}\\n---------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:55.733171Z","iopub.execute_input":"2025-02-13T16:46:55.733653Z","iopub.status.idle":"2025-02-13T16:47:02.761813Z","shell.execute_reply.started":"2025-02-13T16:46:55.733599Z","shell.execute_reply":"2025-02-13T16:47:02.760604Z"}},"outputs":[{"name":"stdout","text":"Informal: Wanna grab a coffee?\nFormal: Wanna grab a coffee?\n---------------------\nInformal: Hey dude, what’s up?\nFormal: Hey dude, what’s up?\n---------------------\nInformal: No way that actually happened!\nFormal: No way that happened!\n---------------------\nInformal: Gimme a min to check.\nFormal: I'll give you a minute to check.\n---------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"optimizer = optim.Adam(t5_model.parameters(), lr=5e-6)\nnum_epochs = 3\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for data in dataset:\n        for _ in range(3):\n            input_text = data[\"input\"]\n\n            inputs = t5_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n            outputs = t5_model.generate(**inputs, max_length=50)\n            generated_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n            print(f\"Informal Text: {input_text}\\nGenerated Formal Text: {generated_text}\")\n            reward_score = float(input(\"Rate the output (0-5): \"))\n    \n            ## policy gradient logics, TO DO\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n\nt5_model.save_pretrained(\"fine_tuned\")\nt5_tokenizer.save_pretrained(\"fine_tuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for sent in test_sentences:\n    print(f\"Informal: {sent}\")\n    print(f\"Formal: {test_model(sent)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:47:04.343488Z","iopub.status.idle":"2025-02-13T16:47:04.343840Z","shell.execute_reply":"2025-02-13T16:47:04.343684Z"}},"outputs":[],"execution_count":null}]}