{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.736378Z","iopub.execute_input":"2025-02-14T21:15:04.736755Z","iopub.status.idle":"2025-02-14T21:15:04.741452Z","shell.execute_reply.started":"2025-02-14T21:15:04.736729Z","shell.execute_reply":"2025-02-14T21:15:04.740127Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n    \n    def reset(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        return np.array(self.board, dtype=np.float32)\n    \n    def get_available_actions(self):\n        return [i for i, x in enumerate(self.board) if x == 0]\n    \n    def make_move(self, action):\n        self.board[action] = self.current_player\n        if self.check_win(self.current_player):\n            return self.current_player, True\n        elif self.is_full():\n            return 0, True\n        else:\n            self.current_player *= -1\n            return None, False\n    \n    def check_win(self, player):\n        win_conditions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n            [0, 4, 8], [2, 4, 6]\n        ]\n        for condition in win_conditions:\n            if all(self.board[i] == player for i in condition):\n                return True\n        return False\n    \n    def is_full(self):\n        return all(x != 0 for x in self.board)\n\n    def render(self):\n        for i in range(0,9,3):\n            print(self.board[i:i+3])\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.744912Z","iopub.execute_input":"2025-02-14T21:15:04.745188Z","iopub.status.idle":"2025-02-14T21:15:04.760989Z","shell.execute_reply.started":"2025-02-14T21:15:04.745166Z","shell.execute_reply":"2025-02-14T21:15:04.759872Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PolicyGradientAgent:\n    def __init__(self, lr=0.01, gamma=0.99):\n        self.gamma = gamma\n        self.model = nn.Sequential(\n            nn.Linear(9, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 9),\n            nn.Softmax(dim=-1)\n        )\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.states, self.actions, self.rewards = [], [], []\n    \n    def choose_action(self, state, available_actions, train=True):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n        action_probs = self.model(state).detach().numpy().flatten()\n        if not train: print(action_probs)\n        action_probs = np.array([action_probs[i] if i in available_actions else 0 for i in range(9)])\n        # action_probs /= np.sum(action_probs)\n        # return np.random.choice(9, p=action_probs)\n        return np.argmax(action_probs)\n    \n    def store_outcome(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n    \n    def train(self):\n        discounted_rewards = self.compute_discounted_rewards()\n        states = torch.tensor(self.states, dtype=torch.float32)\n        actions = torch.tensor(self.actions, dtype=torch.int64)\n        rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n        \n        logits = self.model(states)\n        action_masks = torch.nn.functional.one_hot(actions, num_classes=9)\n        log_probs = torch.sum(action_masks * torch.log(logits + 1e-10), dim=1)\n        loss = -torch.mean(log_probs * rewards)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        self.states, self.actions, self.rewards = [], [], []\n    \n    def compute_discounted_rewards(self):\n        discounted_rewards = np.zeros_like(self.rewards, dtype=np.float32)\n        cumulative_reward = 0\n        for i in reversed(range(len(self.rewards))):\n            cumulative_reward = self.rewards[i] + self.gamma * cumulative_reward\n            discounted_rewards[i] = cumulative_reward\n        return (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.762615Z","iopub.execute_input":"2025-02-14T21:15:04.763035Z","iopub.status.idle":"2025-02-14T21:15:04.783645Z","shell.execute_reply.started":"2025-02-14T21:15:04.762998Z","shell.execute_reply":"2025-02-14T21:15:04.782489Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"In **compute_discounted_rewards**:\n\n- $G_t = R_t + \\gamma G_{t+1}$ or $G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$\n- $\\hat{G_t} = \\dfrac{G_t - \\mu_G}{\\sigma_G + \\epsilon}$ to reduce variance\n\nIn **train**:\n\n- $L(\\theta) = âˆ’\\mathbb{E}\\left[G_t \\cdot \\log \\pi_\\theta (a_t | s_t)\\right] = -\\dfrac{1}{N} \\sum_{t=1}^{T}G_t \\log \\pi_\\theta (a_t | s_t)$\n- Finally, the update will be: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ where $\\nabla_\\theta J(\\theta) = \\sum_{t=1}^{T}G_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)$","metadata":{}},{"cell_type":"code","source":"def evaluate(agent, env, both, num_games=100, render=False):\n    wins, losses, draws = 0, 0, 0\n    for _ in range(num_games):\n        state = env.reset()\n        done = False\n        if render: env.render()\n        while not done:\n            available_actions = env.get_available_actions()\n            if both == False and env.current_player == -1:\n                action = np.random.choice(available_actions)\n            else:\n                action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            state = np.array(env.board, dtype=np.float32)\n            if render: env.render()\n        if reward == 1:\n            wins += 1\n            if render: print(\"WIN!\")\n        elif reward == -1:\n            losses += 1\n            if render: print(\"LOSE!\")\n        else:\n            draws += 1\n            if render: print(\"DRAW!\")\n        if render: print(\"-----------------------\")\n    print(f\"Evaluation Results: Wins: {wins}, Losses: {losses}, Draws: {draws}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.785611Z","iopub.execute_input":"2025-02-14T21:15:04.786105Z","iopub.status.idle":"2025-02-14T21:15:04.807016Z","shell.execute_reply.started":"2025-02-14T21:15:04.785978Z","shell.execute_reply":"2025-02-14T21:15:04.805871Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Train both","metadata":{}},{"cell_type":"code","source":"def train_both(agent, env, num_episodes=5000):\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            available_actions = env.get_available_actions()\n            action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            if env.current_player == -1: \n                reward = reward * -1 if reward is not None else 0\n            agent.store_outcome(state, action, reward if reward is not None else 0)\n            state = np.array(env.board, dtype=np.float32)\n        agent.train()\n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.808646Z","iopub.execute_input":"2025-02-14T21:15:04.808937Z","iopub.status.idle":"2025-02-14T21:15:04.829475Z","shell.execute_reply.started":"2025-02-14T21:15:04.808896Z","shell.execute_reply":"2025-02-14T21:15:04.828486Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"env = TicTacToe()\nagent = PolicyGradientAgent()\ntrain_both(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:04.830327Z","iopub.execute_input":"2025-02-14T21:15:04.830658Z","iopub.status.idle":"2025-02-14T21:15:27.060147Z","shell.execute_reply.started":"2025-02-14T21:15:04.830633Z","shell.execute_reply":"2025-02-14T21:15:27.058998Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-01615867228a>:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  states = torch.tensor(self.states, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Episode 500/5000 completed.\nEpisode 1000/5000 completed.\nEpisode 1500/5000 completed.\nEpisode 2000/5000 completed.\nEpisode 2500/5000 completed.\nEpisode 3000/5000 completed.\nEpisode 3500/5000 completed.\nEpisode 4000/5000 completed.\nEpisode 4500/5000 completed.\nEpisode 5000/5000 completed.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"evaluate(agent, env, both=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:27.061795Z","iopub.execute_input":"2025-02-14T21:15:27.062267Z","iopub.status.idle":"2025-02-14T21:15:27.240310Z","shell.execute_reply.started":"2025-02-14T21:15:27.062239Z","shell.execute_reply":"2025-02-14T21:15:27.239471Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 0, Losses: 0, Draws: 100\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Train one player","metadata":{}},{"cell_type":"code","source":"def train_one(agent, env, num_episodes=6000):\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            available_actions = env.get_available_actions()\n            if env.current_player == -1:\n                action = np.random.choice(available_actions)\n            else:\n                action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            if env.current_player == 1:\n                agent.store_outcome(state, action, reward if reward is not None else 0)\n            state = np.array(env.board, dtype=np.float32)\n        agent.train()\n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:27.241555Z","iopub.execute_input":"2025-02-14T21:15:27.241845Z","iopub.status.idle":"2025-02-14T21:15:27.249012Z","shell.execute_reply.started":"2025-02-14T21:15:27.241808Z","shell.execute_reply":"2025-02-14T21:15:27.247893Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"env = TicTacToe()\nagent = PolicyGradientAgent()\ntrain_one(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:27.251051Z","iopub.execute_input":"2025-02-14T21:15:27.251357Z","iopub.status.idle":"2025-02-14T21:15:44.189763Z","shell.execute_reply.started":"2025-02-14T21:15:27.251332Z","shell.execute_reply":"2025-02-14T21:15:44.188469Z"}},"outputs":[{"name":"stdout","text":"Episode 500/6000 completed.\nEpisode 1000/6000 completed.\nEpisode 1500/6000 completed.\nEpisode 2000/6000 completed.\nEpisode 2500/6000 completed.\nEpisode 3000/6000 completed.\nEpisode 3500/6000 completed.\nEpisode 4000/6000 completed.\nEpisode 4500/6000 completed.\nEpisode 5000/6000 completed.\nEpisode 5500/6000 completed.\nEpisode 6000/6000 completed.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"evaluate(agent, env, both=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:15:44.191164Z","iopub.execute_input":"2025-02-14T21:15:44.191601Z","iopub.status.idle":"2025-02-14T21:15:44.289628Z","shell.execute_reply.started":"2025-02-14T21:15:44.191559Z","shell.execute_reply":"2025-02-14T21:15:44.288500Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 76, Losses: 17, Draws: 7\n","output_type":"stream"}],"execution_count":11}]}