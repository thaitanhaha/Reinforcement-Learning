{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:55:13.509895Z","iopub.execute_input":"2025-02-13T10:55:13.510401Z","iopub.status.idle":"2025-02-13T10:55:15.416009Z","shell.execute_reply.started":"2025-02-13T10:55:13.510358Z","shell.execute_reply":"2025-02-13T10:55:15.414949Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n    \n    def reset(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        return np.array(self.board, dtype=np.float32)\n    \n    def get_available_actions(self):\n        return [i for i, x in enumerate(self.board) if x == 0]\n    \n    def make_move(self, action):\n        self.board[action] = self.current_player\n        if self.check_win(self.current_player):\n            return self.current_player, True\n        elif self.is_full():\n            return 0, True\n        else:\n            self.current_player *= -1\n            return None, False\n    \n    def check_win(self, player):\n        win_conditions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n            [0, 4, 8], [2, 4, 6]\n        ]\n        for condition in win_conditions:\n            if all(self.board[i] == player for i in condition):\n                return True\n        return False\n    \n    def is_full(self):\n        return all(x != 0 for x in self.board)\n\n    def render(self):\n        for i in range(0,9,3):\n            print(self.board[i:i+3])\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:55:15.417491Z","iopub.execute_input":"2025-02-13T10:55:15.417917Z","iopub.status.idle":"2025-02-13T10:55:15.427009Z","shell.execute_reply.started":"2025-02-13T10:55:15.417886Z","shell.execute_reply":"2025-02-13T10:55:15.425603Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PolicyGradientAgent:\n    def __init__(self, lr=0.01, gamma=0.99):\n        self.gamma = gamma\n        self.model = nn.Sequential(\n            nn.Linear(9, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 9),\n            nn.Softmax(dim=-1)\n        )\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.states, self.actions, self.rewards = [], [], []\n    \n    def choose_action(self, state, available_actions, train=True):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n        action_probs = self.model(state).detach().numpy().flatten()\n        if not train: print(action_probs)\n        action_probs = np.array([action_probs[i] if i in available_actions else 0 for i in range(9)])\n        # action_probs /= np.sum(action_probs)\n        # return np.random.choice(9, p=action_probs)\n        return np.argmax(action_probs)\n    \n    def store_outcome(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n    \n    def train(self):\n        discounted_rewards = self.compute_discounted_rewards()\n        states = torch.tensor(self.states, dtype=torch.float32)\n        actions = torch.tensor(self.actions, dtype=torch.int64)\n        rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n        \n        logits = self.model(states)\n        action_masks = torch.nn.functional.one_hot(actions, num_classes=9)\n        log_probs = torch.sum(action_masks * torch.log(logits + 1e-10), dim=1)\n        loss = -torch.mean(log_probs * rewards)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        self.states, self.actions, self.rewards = [], [], []\n    \n    def compute_discounted_rewards(self):\n        discounted_rewards = np.zeros_like(self.rewards, dtype=np.float32)\n        cumulative_reward = 0\n        for i in reversed(range(len(self.rewards))):\n            cumulative_reward = self.rewards[i] + self.gamma * cumulative_reward\n            discounted_rewards[i] = cumulative_reward\n        return (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:55:15.428948Z","iopub.execute_input":"2025-02-13T10:55:15.429340Z","iopub.status.idle":"2025-02-13T10:55:15.451784Z","shell.execute_reply.started":"2025-02-13T10:55:15.429307Z","shell.execute_reply":"2025-02-13T10:55:15.450795Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"In **compute_discounted_rewards**:\n\n- $G_t = R_t + \\gamma G_{t+1}$ or $G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$\n- $\\hat{G_t} = \\dfrac{G_t - \\mu_G}{\\sigma_G + \\epsilon}$ to reduce variance\n\nIn **train**:\n\n- $L(\\theta) = âˆ’\\mathbb{E}\\left[G_t \\cdot \\log \\pi_\\theta (a_t | s_t)\\right] = -\\dfrac{1}{N} \\sum_{t=1}^{T}G_t \\log \\pi_\\theta (a_t | s_t)$\n- Finally, the update will be: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ where $\\nabla_\\theta J(\\theta) = \\sum_{t=1}^{T}G_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)$","metadata":{}},{"cell_type":"code","source":"def train(agent, env, both, num_episodes=5000):\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            available_actions = env.get_available_actions()\n            if both == False and env.current_player == -1:\n                action = np.random.choice(available_actions)\n            else:\n                action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            agent.store_outcome(state, action, reward if reward is not None else 0)\n            state = np.array(env.board, dtype=np.float32)\n        agent.train()\n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:55:15.453046Z","iopub.execute_input":"2025-02-13T10:55:15.453413Z","iopub.status.idle":"2025-02-13T10:55:15.473265Z","shell.execute_reply.started":"2025-02-13T10:55:15.453362Z","shell.execute_reply":"2025-02-13T10:55:15.472224Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def evaluate(agent, env, num_games=100, render=False):\n    wins, losses, draws = 0, 0, 0\n    for _ in range(num_games):\n        state = env.reset()\n        done = False\n        if render: env.render()\n        while not done:\n            available_actions = env.get_available_actions()\n            action = agent.choose_action(state, available_actions, True)\n            reward, done = env.make_move(action)\n            state = np.array(env.board, dtype=np.float32)\n            if render: env.render()\n        if reward == 1:\n            wins += 1\n            if render: print(\"WIN!\")\n        elif reward == -1:\n            losses += 1\n            if render: print(\"LOSE!\")\n        else:\n            draws += 1\n            if render: print(\"DRAW!\")\n        if render: print(\"-----------------------\")\n    print(f\"Evaluation Results: Wins: {wins}, Losses: {losses}, Draws: {draws}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:56:09.985377Z","iopub.execute_input":"2025-02-13T10:56:09.985721Z","iopub.status.idle":"2025-02-13T10:56:09.992712Z","shell.execute_reply.started":"2025-02-13T10:56:09.985694Z","shell.execute_reply":"2025-02-13T10:56:09.991603Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Train both","metadata":{}},{"cell_type":"code","source":"env = TicTacToe()\nagent = PolicyGradientAgent()\ntrain(agent, env, both=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:55:15.474298Z","iopub.execute_input":"2025-02-13T10:55:15.474658Z","iopub.status.idle":"2025-02-13T10:55:34.865294Z","shell.execute_reply.started":"2025-02-13T10:55:15.474627Z","shell.execute_reply":"2025-02-13T10:55:34.863914Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-01615867228a>:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  states = torch.tensor(self.states, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Episode 500/5000 completed.\nEpisode 1000/5000 completed.\nEpisode 1500/5000 completed.\nEpisode 2000/5000 completed.\nEpisode 2500/5000 completed.\nEpisode 3000/5000 completed.\nEpisode 3500/5000 completed.\nEpisode 4000/5000 completed.\nEpisode 4500/5000 completed.\nEpisode 5000/5000 completed.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"evaluate(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:56:14.785468Z","iopub.execute_input":"2025-02-13T10:56:14.785817Z","iopub.status.idle":"2025-02-13T10:56:14.945474Z","shell.execute_reply.started":"2025-02-13T10:56:14.785789Z","shell.execute_reply":"2025-02-13T10:56:14.944544Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 0, Losses: 0, Draws: 100\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Train one player","metadata":{}},{"cell_type":"code","source":"env = TicTacToe()\nagent = PolicyGradientAgent()\ntrain(agent, env, both=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:56:17.458289Z","iopub.execute_input":"2025-02-13T10:56:17.458675Z","iopub.status.idle":"2025-02-13T10:56:31.491532Z","shell.execute_reply.started":"2025-02-13T10:56:17.458642Z","shell.execute_reply":"2025-02-13T10:56:31.490500Z"}},"outputs":[{"name":"stdout","text":"Episode 500/5000 completed.\nEpisode 1000/5000 completed.\nEpisode 1500/5000 completed.\nEpisode 2000/5000 completed.\nEpisode 2500/5000 completed.\nEpisode 3000/5000 completed.\nEpisode 3500/5000 completed.\nEpisode 4000/5000 completed.\nEpisode 4500/5000 completed.\nEpisode 5000/5000 completed.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"evaluate(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:57:22.069827Z","iopub.execute_input":"2025-02-13T10:57:22.070212Z","iopub.status.idle":"2025-02-13T10:57:22.196503Z","shell.execute_reply.started":"2025-02-13T10:57:22.070182Z","shell.execute_reply":"2025-02-13T10:57:22.194945Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 100, Losses: 0, Draws: 0\n","output_type":"stream"}],"execution_count":10}]}