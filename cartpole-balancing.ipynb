{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee1e1f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:09:29.003380Z",
     "iopub.status.busy": "2025-02-03T20:09:29.003048Z",
     "iopub.status.idle": "2025-02-03T20:09:29.008180Z",
     "shell.execute_reply": "2025-02-03T20:09:29.007342Z"
    },
    "papermill": {
     "duration": 0.010324,
     "end_time": "2025-02-03T20:09:29.009548",
     "exception": false,
     "start_time": "2025-02-03T20:09:28.999224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e604a507",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-03T20:09:29.014949Z",
     "iopub.status.busy": "2025-02-03T20:09:29.014642Z",
     "iopub.status.idle": "2025-02-03T20:09:29.022569Z",
     "shell.execute_reply": "2025-02-03T20:09:29.021577Z"
    },
    "papermill": {
     "duration": 0.012132,
     "end_time": "2025-02-03T20:09:29.024185",
     "exception": false,
     "start_time": "2025-02-03T20:09:29.012053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CartPoleEnv:\n",
    "    def __init__(self):\n",
    "        # State: [x, x_velocity, theta, theta_velocity]\n",
    "        # Action: 0 left, 1 right\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.g = 9.8\n",
    "        self.m = 0.1\n",
    "        self.M = 1.0\n",
    "        self.L = 0.5\n",
    "        self.dt = 0.02\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, x_velocity, theta, theta_velocity = self.state\n",
    "        \n",
    "        force = 20.0 if action == 1 else -20.0\n",
    "        \n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        total_mass = self.M + self.m\n",
    "        pole_mass_length = self.m * self.L\n",
    "        \n",
    "        temp = (force + pole_mass_length * theta_velocity ** 2 * sin_theta) / total_mass\n",
    "        theta_acc = (self.g * sin_theta - cos_theta * temp) / (self.L * (4 / 3 - self.m * cos_theta ** 2 / total_mass))\n",
    "        x_acc = temp - pole_mass_length * theta_acc * cos_theta / total_mass\n",
    "        \n",
    "        x += x_velocity * self.dt\n",
    "        x_velocity += x_acc * self.dt\n",
    "        theta += theta_velocity * self.dt\n",
    "        theta_velocity += theta_acc * self.dt\n",
    "        \n",
    "        lose = x < -2.4 or x > 2.4 or theta < -np.pi / 15 or theta > np.pi / 15\n",
    "        reward = 1.0 if not lose else 0.0\n",
    "        \n",
    "        self.state = np.array([x, x_velocity, theta, theta_velocity])\n",
    "        \n",
    "        return self.state, reward, lose\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f30f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:09:29.029322Z",
     "iopub.status.busy": "2025-02-03T20:09:29.029085Z",
     "iopub.status.idle": "2025-02-03T20:09:29.036041Z",
     "shell.execute_reply": "2025-02-03T20:09:29.035286Z"
    },
    "papermill": {
     "duration": 0.011014,
     "end_time": "2025-02-03T20:09:29.037363",
     "exception": false,
     "start_time": "2025-02-03T20:09:29.026349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, state_space, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros(state_space + (action_space,))\n",
    "        self.state_bins = [\n",
    "            np.linspace(-2.4, 2.4, 10), # x position\n",
    "            np.linspace(-3.0, 3.0, 10), # x velocity\n",
    "            np.linspace(-np.pi / 15, np.pi / 15, 10), # theta\n",
    "            np.linspace(-3.0, 3.0, 10), # theta velocity\n",
    "        ]\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        # Discretize the continuous state space into bins\n",
    "        discretized = []\n",
    "        for i in range(len(state)):\n",
    "            discretized.append(np.digitize(state[i], self.state_bins[i]) - 1)\n",
    "        \n",
    "        return tuple(discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state + (best_next_action,)]\n",
    "        td_error = td_target - self.q_table[state + (action,)]\n",
    "        self.q_table[state + (action,)] += self.alpha * td_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff1b193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:09:29.042262Z",
     "iopub.status.busy": "2025-02-03T20:09:29.042028Z",
     "iopub.status.idle": "2025-02-03T20:09:29.047134Z",
     "shell.execute_reply": "2025-02-03T20:09:29.046466Z"
    },
    "papermill": {
     "duration": 0.009062,
     "end_time": "2025-02-03T20:09:29.048502",
     "exception": false,
     "start_time": "2025-02-03T20:09:29.039440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, env, episodes, max_timesteps=200):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = agent.discretize_state(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, lose = env.step(action)\n",
    "            next_state = agent.discretize_state(next_state)\n",
    "            \n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if lose: break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        if episode % 500 == 0 or episode == episodes-1:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9feae8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:09:29.054395Z",
     "iopub.status.busy": "2025-02-03T20:09:29.054151Z",
     "iopub.status.idle": "2025-02-03T20:10:21.299929Z",
     "shell.execute_reply": "2025-02-03T20:10:21.298879Z"
    },
    "papermill": {
     "duration": 52.251187,
     "end_time": "2025-02-03T20:10:21.301584",
     "exception": false,
     "start_time": "2025-02-03T20:09:29.050397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 6.0\n",
      "Episode 500, Total Reward: 8.0\n",
      "Episode 1000, Total Reward: 38.0\n",
      "Episode 1500, Total Reward: 75.0\n",
      "Episode 2000, Total Reward: 122.0\n",
      "Episode 2500, Total Reward: 117.0\n",
      "Episode 3000, Total Reward: 75.0\n",
      "Episode 3500, Total Reward: 200.0\n",
      "Episode 4000, Total Reward: 172.0\n",
      "Episode 4500, Total Reward: 90.0\n",
      "Episode 5000, Total Reward: 143.0\n",
      "Episode 5500, Total Reward: 61.0\n",
      "Episode 6000, Total Reward: 126.0\n",
      "Episode 6500, Total Reward: 200.0\n",
      "Episode 7000, Total Reward: 200.0\n",
      "Episode 7500, Total Reward: 200.0\n",
      "Episode 8000, Total Reward: 200.0\n",
      "Episode 8500, Total Reward: 200.0\n",
      "Episode 9000, Total Reward: 158.0\n",
      "Episode 9500, Total Reward: 200.0\n",
      "Episode 9999, Total Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "env = CartPoleEnv()\n",
    "agent = QLearningAgent(action_space=2, state_space=(10, 10, 10, 10))\n",
    "rewards = train(agent, env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32610b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:10:21.309162Z",
     "iopub.status.busy": "2025-02-03T20:10:21.308854Z",
     "iopub.status.idle": "2025-02-03T20:10:23.062969Z",
     "shell.execute_reply": "2025-02-03T20:10:23.061729Z"
    },
    "papermill": {
     "duration": 1.759463,
     "end_time": "2025-02-03T20:10:23.064450",
     "exception": false,
     "start_time": "2025-02-03T20:10:21.304987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 335.0\n",
      "Episode 1, Total Reward: 4611.0\n",
      "Episode 2, Total Reward: 629.0\n",
      "Episode 3, Total Reward: 575.0\n",
      "Episode 4, Total Reward: 22240.0\n",
      "Episode 5, Total Reward: 5387.0\n",
      "Episode 6, Total Reward: 636.0\n",
      "Episode 7, Total Reward: 3175.0\n",
      "Episode 8, Total Reward: 7030.0\n",
      "Episode 9, Total Reward: 11094.0\n"
     ]
    }
   ],
   "source": [
    "def render(env):\n",
    "    x, x_dot, theta, theta_dot = env.state\n",
    "    \n",
    "    cart_width = 0.4\n",
    "    cart_height = 0.2\n",
    "    pole_length = 0.5\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 1)\n",
    "    \n",
    "    ax.add_patch(plt.Rectangle((-cart_width / 2 + x, -cart_height / 2), cart_width, cart_height, color='blue'))\n",
    "    \n",
    "    pole_x = x + pole_length * np.sin(theta)\n",
    "    pole_y = pole_length * np.cos(theta)\n",
    "    ax.plot([x, pole_x], [0, pole_y], lw=5, color='red')\n",
    "\n",
    "    plt.pause(0.02)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "def evaluate(agent, env, episodes, is_render):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = agent.discretize_state(state)\n",
    "        lose = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not lose:\n",
    "            action = np.argmax(agent.q_table[state])\n",
    "            state, reward, lose = env.step(action)\n",
    "            state = agent.discretize_state(state)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if is_render: render(env)\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "\n",
    "evaluate(agent, env, 10, False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57.024902,
   "end_time": "2025-02-03T20:10:23.486692",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T20:09:26.461790",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
