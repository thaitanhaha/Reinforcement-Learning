{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:36.272635Z","iopub.execute_input":"2025-02-14T20:59:36.273175Z","iopub.status.idle":"2025-02-14T20:59:40.853657Z","shell.execute_reply.started":"2025-02-14T20:59:36.273130Z","shell.execute_reply":"2025-02-14T20:59:40.852427Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n    \n    def reset(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        return np.array(self.board, dtype=np.float32)\n    \n    def get_available_actions(self):\n        return [i for i, x in enumerate(self.board) if x == 0]\n    \n    def make_move(self, action):\n        self.board[action] = self.current_player\n        if self.check_win(self.current_player):\n            return self.current_player, True\n        elif self.is_full():\n            return 0, True\n        else:\n            self.current_player *= -1\n            return None, False\n    \n    def check_win(self, player):\n        win_conditions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n            [0, 4, 8], [2, 4, 6]\n        ]\n        for condition in win_conditions:\n            if all(self.board[i] == player for i in condition):\n                return True\n        return False\n    \n    def is_full(self):\n        return all(x != 0 for x in self.board)\n\n    def render(self):\n        for i in range(0,9,3):\n            print(self.board[i:i+3])\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:40.855092Z","iopub.execute_input":"2025-02-14T20:59:40.855664Z","iopub.status.idle":"2025-02-14T20:59:40.865977Z","shell.execute_reply.started":"2025-02-14T20:59:40.855624Z","shell.execute_reply":"2025-02-14T20:59:40.864275Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ActorCriticAgent(nn.Module):\n    def __init__(self, lr=0.01, gamma=0.99):\n        super(ActorCriticAgent, self).__init__()\n        self.gamma = gamma\n        self.actor = nn.Sequential(\n            nn.Linear(9, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 9),\n            nn.Softmax(dim=-1)\n        )\n        self.critic = nn.Sequential(\n            nn.Linear(9, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n        self.states, self.actions, self.rewards = [], [], []\n    \n    def choose_action(self, state, available_actions):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n        action_probs = self.actor(state).detach().numpy().flatten()\n        action_probs = np.array([action_probs[i] if i in available_actions else 0 for i in range(9)])\n        return np.argmax(action_probs)\n    \n    def store_outcome(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n    \n    def train(self):\n        discounted_rewards = self.compute_discounted_rewards()\n        states = torch.tensor(self.states, dtype=torch.float32)\n        actions = torch.tensor(self.actions, dtype=torch.int64)\n        rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n        \n        values = self.critic(states).squeeze()\n        advantages = rewards - values.detach()\n        \n        logits = self.actor(states)\n        action_masks = torch.nn.functional.one_hot(actions, num_classes=9)\n        log_probs = torch.sum(action_masks * torch.log(logits + 1e-10), dim=1)\n        \n        actor_loss = -torch.mean(log_probs * advantages)\n        critic_loss = torch.mean((rewards - values) ** 2)\n        loss = actor_loss + critic_loss\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        self.states, self.actions, self.rewards = [], [], []\n    \n    def compute_discounted_rewards(self):\n        discounted_rewards = np.zeros_like(self.rewards, dtype=np.float32)\n        cumulative_reward = 0\n        for i in reversed(range(len(self.rewards))):\n            cumulative_reward = self.rewards[i] + self.gamma * cumulative_reward\n            discounted_rewards[i] = cumulative_reward\n        return (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:40.868999Z","iopub.execute_input":"2025-02-14T20:59:40.869575Z","iopub.status.idle":"2025-02-14T20:59:40.934546Z","shell.execute_reply.started":"2025-02-14T20:59:40.869524Z","shell.execute_reply":"2025-02-14T20:59:40.933237Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def evaluate(agent, env, both, num_games=100, render=False):\n    wins, losses, draws = 0, 0, 0\n    for _ in range(num_games):\n        state = env.reset()\n        done = False\n        if render: env.render()\n        while not done:\n            available_actions = env.get_available_actions()\n            if both == False and env.current_player == -1:\n                action = np.random.choice(available_actions)\n            else:\n                action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            state = np.array(env.board, dtype=np.float32)\n            if render: env.render()\n        if reward == 1:\n            wins += 1\n            if render: print(\"WIN!\")\n        elif reward == -1:\n            losses += 1\n            if render: print(\"LOSE!\")\n        else:\n            draws += 1\n            if render: print(\"DRAW!\")\n        if render: print(\"-----------------------\")\n    print(f\"Evaluation Results: Wins: {wins}, Losses: {losses}, Draws: {draws}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:40.936541Z","iopub.execute_input":"2025-02-14T20:59:40.936944Z","iopub.status.idle":"2025-02-14T20:59:40.962377Z","shell.execute_reply.started":"2025-02-14T20:59:40.936905Z","shell.execute_reply":"2025-02-14T20:59:40.961271Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Train both","metadata":{}},{"cell_type":"code","source":"def train_both(agent, env, num_episodes=5000):\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            available_actions = env.get_available_actions()\n            action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            if env.current_player == -1: \n                reward = reward * -1 if reward is not None else 0\n            agent.store_outcome(state, action, reward if reward is not None else 0)\n            state = np.array(env.board, dtype=np.float32)\n        agent.train()\n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:40.963510Z","iopub.execute_input":"2025-02-14T20:59:40.963893Z","iopub.status.idle":"2025-02-14T20:59:40.991199Z","shell.execute_reply.started":"2025-02-14T20:59:40.963859Z","shell.execute_reply":"2025-02-14T20:59:40.989736Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"env = TicTacToe()\nagent = ActorCriticAgent()\ntrain_both(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T20:59:40.992878Z","iopub.execute_input":"2025-02-14T20:59:40.993331Z","iopub.status.idle":"2025-02-14T21:00:13.386865Z","shell.execute_reply.started":"2025-02-14T20:59:40.993290Z","shell.execute_reply":"2025-02-14T21:00:13.385389Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-75a7581ee9c8>:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  states = torch.tensor(self.states, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Episode 500/5000 completed.\nEpisode 1000/5000 completed.\nEpisode 1500/5000 completed.\nEpisode 2000/5000 completed.\nEpisode 2500/5000 completed.\nEpisode 3000/5000 completed.\nEpisode 3500/5000 completed.\nEpisode 4000/5000 completed.\nEpisode 4500/5000 completed.\nEpisode 5000/5000 completed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"evaluate(agent, env, both=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:00:13.388515Z","iopub.execute_input":"2025-02-14T21:00:13.389136Z","iopub.status.idle":"2025-02-14T21:00:13.590897Z","shell.execute_reply.started":"2025-02-14T21:00:13.389094Z","shell.execute_reply":"2025-02-14T21:00:13.589306Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 0, Losses: 0, Draws: 100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Train one player","metadata":{}},{"cell_type":"code","source":"def train_one(agent, env, num_episodes=6000):\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            available_actions = env.get_available_actions()\n            if env.current_player == -1:\n                action = np.random.choice(available_actions)\n            else:\n                action = agent.choose_action(state, available_actions)\n            reward, done = env.make_move(action)\n            if env.current_player == 1:\n                agent.store_outcome(state, action, reward if reward is not None else 0)\n            state = np.array(env.board, dtype=np.float32)\n        agent.train()\n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:10:41.589527Z","iopub.execute_input":"2025-02-14T21:10:41.589969Z","iopub.status.idle":"2025-02-14T21:10:41.597464Z","shell.execute_reply.started":"2025-02-14T21:10:41.589938Z","shell.execute_reply":"2025-02-14T21:10:41.596174Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"env = TicTacToe()\nagent = ActorCriticAgent()\ntrain_one(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:10:43.222412Z","iopub.execute_input":"2025-02-14T21:10:43.222819Z","iopub.status.idle":"2025-02-14T21:11:10.839501Z","shell.execute_reply.started":"2025-02-14T21:10:43.222765Z","shell.execute_reply":"2025-02-14T21:11:10.838150Z"}},"outputs":[{"name":"stdout","text":"Episode 500/6000 completed.\nEpisode 1000/6000 completed.\nEpisode 1500/6000 completed.\nEpisode 2000/6000 completed.\nEpisode 2500/6000 completed.\nEpisode 3000/6000 completed.\nEpisode 3500/6000 completed.\nEpisode 4000/6000 completed.\nEpisode 4500/6000 completed.\nEpisode 5000/6000 completed.\nEpisode 5500/6000 completed.\nEpisode 6000/6000 completed.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"evaluate(agent, env, both=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:11:10.842361Z","iopub.execute_input":"2025-02-14T21:11:10.842871Z","iopub.status.idle":"2025-02-14T21:11:10.956111Z","shell.execute_reply.started":"2025-02-14T21:11:10.842822Z","shell.execute_reply":"2025-02-14T21:11:10.954569Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 95, Losses: 5, Draws: 0\n","output_type":"stream"}],"execution_count":29}]}