{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom collections import deque","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:22:52.674289Z","iopub.execute_input":"2025-02-11T16:22:52.674577Z","iopub.status.idle":"2025-02-11T16:22:57.104627Z","shell.execute_reply.started":"2025-02-11T16:22:52.674552Z","shell.execute_reply":"2025-02-11T16:22:57.103195Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class CartPoleEnv:\n    def __init__(self):\n        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n        self.g = 9.8\n        self.m = 0.1\n        self.M = 1.0\n        self.L = 0.5\n        self.dt = 0.02\n        \n    def step(self, action):\n        x, x_velocity, theta, theta_velocity = self.state\n        force = 20.0 if action == 1 else -20.0\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n        total_mass = self.M + self.m\n        pole_mass_length = self.m * self.L\n        temp = (force + pole_mass_length * theta_velocity ** 2 * sin_theta) / total_mass\n        theta_acc = (self.g * sin_theta - cos_theta * temp) / (self.L * (4 / 3 - self.m * cos_theta ** 2 / total_mass))\n        x_acc = temp - pole_mass_length * theta_acc * cos_theta / total_mass\n        x += x_velocity * self.dt\n        x_velocity += x_acc * self.dt\n        theta += theta_velocity * self.dt\n        theta_velocity += theta_acc * self.dt\n        self.state = np.array([x, x_velocity, theta, theta_velocity])\n        reward = 1.0 if abs(theta) < np.pi / 12 else -1.0\n        done = abs(theta) >= np.pi / 6\n        return self.state, reward, done\n    \n    def reset(self):\n        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n        return self.state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:22:57.105821Z","iopub.execute_input":"2025-02-11T16:22:57.106300Z","iopub.status.idle":"2025-02-11T16:22:57.116025Z","shell.execute_reply.started":"2025-02-11T16:22:57.106258Z","shell.execute_reply":"2025-02-11T16:22:57.114451Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 24)\n        self.fc2 = nn.Linear(24, 24)\n        self.fc3 = nn.Linear(24, action_dim)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:22:57.116991Z","iopub.execute_input":"2025-02-11T16:22:57.117588Z","iopub.status.idle":"2025-02-11T16:22:57.140248Z","shell.execute_reply.started":"2025-02-11T16:22:57.117548Z","shell.execute_reply":"2025-02-11T16:22:57.139027Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.batch_size = 32\n        self.model = DQN(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n        self.criterion = nn.MSELoss()\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        state = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            q_values = self.model(state)\n        return torch.argmax(q_values).item()\n    \n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        minibatch = random.sample(self.memory, self.batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target = reward\n            if not done:\n                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n                target += self.gamma * torch.max(self.model(next_state)).item()\n            state = torch.FloatTensor(state).unsqueeze(0)\n            target_f = self.model(state).clone().detach()\n            target_f[0][action] = target\n            self.optimizer.zero_grad()\n            output = self.model(state)\n            loss = self.criterion(output, target_f)\n            loss.backward()\n            self.optimizer.step()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:22:57.142394Z","iopub.execute_input":"2025-02-11T16:22:57.142727Z","iopub.status.idle":"2025-02-11T16:22:57.169619Z","shell.execute_reply.started":"2025-02-11T16:22:57.142695Z","shell.execute_reply":"2025-02-11T16:22:57.168284Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"env = CartPoleEnv()\nagent = DQNAgent(state_dim=4, action_dim=2)\nepisodes = 1000\n\nfor e in range(episodes):\n    state = env.reset()\n    total_reward = 0\n    for time in range(200):\n        action = agent.act(state)\n        next_state, reward, done = env.step(action)\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n        if done: break\n    agent.replay()\n    if (e+1)%50 == 0: print(f\"Episode {e+1}/{episodes}, Reward: {total_reward}, Epsilon: {agent.epsilon:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:22:57.171010Z","iopub.execute_input":"2025-02-11T16:22:57.171428Z","iopub.status.idle":"2025-02-11T16:24:06.651983Z","shell.execute_reply.started":"2025-02-11T16:22:57.171395Z","shell.execute_reply":"2025-02-11T16:24:06.650402Z"}},"outputs":[{"name":"stdout","text":"Episode 50/1000, Reward: 8.0, Epsilon: 0.7862\nEpisode 100/1000, Reward: 8.0, Epsilon: 0.6119\nEpisode 150/1000, Reward: 19.0, Epsilon: 0.4762\nEpisode 200/1000, Reward: 22.0, Epsilon: 0.3707\nEpisode 250/1000, Reward: 2.0, Epsilon: 0.2885\nEpisode 300/1000, Reward: 3.0, Epsilon: 0.2245\nEpisode 350/1000, Reward: 9.0, Epsilon: 0.1748\nEpisode 400/1000, Reward: -1.0, Epsilon: 0.1360\nEpisode 450/1000, Reward: 200.0, Epsilon: 0.1059\nEpisode 500/1000, Reward: 200.0, Epsilon: 0.0824\nEpisode 550/1000, Reward: 200.0, Epsilon: 0.0641\nEpisode 600/1000, Reward: 200.0, Epsilon: 0.0499\nEpisode 650/1000, Reward: 200.0, Epsilon: 0.0388\nEpisode 700/1000, Reward: 200.0, Epsilon: 0.0302\nEpisode 750/1000, Reward: 200.0, Epsilon: 0.0235\nEpisode 800/1000, Reward: 200.0, Epsilon: 0.0183\nEpisode 850/1000, Reward: 200.0, Epsilon: 0.0143\nEpisode 900/1000, Reward: 2.0, Epsilon: 0.0111\nEpisode 950/1000, Reward: 200.0, Epsilon: 0.0100\nEpisode 1000/1000, Reward: 200.0, Epsilon: 0.0100\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def evaluate(agent, env, episodes=10):\n    total_rewards = []\n    for i in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        lose = False\n        while not lose:\n            action = agent.act(state)\n            state, reward, lose = env.step(action)\n            total_reward += reward\n        print(f'Episode {i}: {total_reward}')\n\nevaluate(agent, env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:25:00.496799Z","iopub.execute_input":"2025-02-11T16:25:00.497216Z","iopub.status.idle":"2025-02-11T16:25:01.073036Z","shell.execute_reply.started":"2025-02-11T16:25:00.497189Z","shell.execute_reply":"2025-02-11T16:25:01.071431Z"}},"outputs":[{"name":"stdout","text":"Episode 0: 397.0\nEpisode 1: 377.0\nEpisode 2: 306.0\nEpisode 3: 291.0\nEpisode 4: 354.0\nEpisode 5: 345.0\nEpisode 6: 343.0\nEpisode 7: 339.0\nEpisode 8: 299.0\nEpisode 9: 289.0\n","output_type":"stream"}],"execution_count":7}]}