{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.619303Z","iopub.execute_input":"2025-02-14T21:38:00.619675Z","iopub.status.idle":"2025-02-14T21:38:00.623936Z","shell.execute_reply.started":"2025-02-14T21:38:00.619643Z","shell.execute_reply":"2025-02-14T21:38:00.622910Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n    \n    def reset(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        return self.board\n    \n    def get_available_actions(self):\n        return [i for i, x in enumerate(self.board) if x == 0]\n    \n    def make_move(self, action):\n        self.board[action] = self.current_player\n        if self.check_win(self.current_player):\n            return self.current_player\n        elif self.is_full():\n            return 0\n        else:\n            self.current_player *= -1\n            return None\n    \n    def check_win(self, player):\n        win_conditions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n            [0, 4, 8], [2, 4, 6]\n        ]\n        for condition in win_conditions:\n            if all(self.board[i] == player for i in condition):\n                return True\n        return False\n    \n    def is_full(self):\n        return all(x != 0 for x in self.board)\n\n    def render(self):\n        for i in range(0, 9, 3):\n            print(self.board[i:i+3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.636571Z","iopub.execute_input":"2025-02-14T21:38:00.636961Z","iopub.status.idle":"2025-02-14T21:38:00.656952Z","shell.execute_reply.started":"2025-02-14T21:38:00.636930Z","shell.execute_reply":"2025-02-14T21:38:00.656006Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class QLearningAgent:\n    def __init__(self, environment, alpha=0.1, gamma=0.9, epsilon=0.1):\n        self.env = environment\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.q_table = {}\n        \n    def get_state(self, board):\n        return tuple(self.env.board)\n    \n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.choice(self.env.get_available_actions())\n        else:\n            if state not in self.q_table:\n                self.q_table[state] = np.zeros(9)\n            available_actions = self.env.get_available_actions()\n            q_values = [self.q_table[state][i] if i in available_actions else -np.inf for i in range(9)]\n            max_q_value = np.max(q_values)\n            max_actions = [i for i in range(9) if q_values[i] == max_q_value]\n            chosen_action = np.random.choice(max_actions)\n            return available_actions[available_actions.index(chosen_action)]\n    \n    def update_q_table(self, state, action, reward, next_state):\n        if next_state not in self.q_table:\n            self.q_table[next_state] = np.zeros(9)\n        self.q_table[state][action] += self.alpha * (\n            reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state][action]\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.658028Z","iopub.execute_input":"2025-02-14T21:38:00.658297Z","iopub.status.idle":"2025-02-14T21:38:00.669382Z","shell.execute_reply.started":"2025-02-14T21:38:00.658275Z","shell.execute_reply":"2025-02-14T21:38:00.668211Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Train one player","metadata":{}},{"cell_type":"code","source":"def train(agent, num_episodes=5000):\n    for episode in range(num_episodes):\n        state = agent.env.reset()\n        done = False\n        while not done:\n            if agent.env.current_player == -1:\n                action = np.random.choice(env.get_available_actions())\n            else:\n                action = agent.choose_action(agent.get_state(state))\n            reward = agent.env.make_move(action)\n            next_state = state.copy()\n            if reward is not None:\n                done = True\n                agent.update_q_table(agent.get_state(state), action, reward, agent.get_state(next_state))\n            state = next_state\n    \n        if (episode + 1) % 500 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.670597Z","iopub.execute_input":"2025-02-14T21:38:00.670919Z","iopub.status.idle":"2025-02-14T21:38:00.688916Z","shell.execute_reply.started":"2025-02-14T21:38:00.670882Z","shell.execute_reply":"2025-02-14T21:38:00.687811Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def evaluate(agent, num_episodes=100, render=False):\n    wins, losses, draws = 0, 0, 0\n    for _ in range(num_episodes):\n        state = agent.env.reset()\n        done = False\n        while not done:\n            if render: agent.env.render()\n            action = agent.choose_action(agent.get_state(state))\n            if render: print(f\"Player {1 if agent.env.current_player == 1 else -1} chooses position {action}\")\n            reward = agent.env.make_move(action)\n            if reward is not None:\n                if render: agent.env.render()\n                if reward == 1:\n                    wins += 1\n                    if render: print(\"WIN!\")\n                elif reward == -1:\n                    losses += 1\n                    if render: print(\"LOSE!\")\n                elif reward == 0:\n                    draws += 1\n                    if render: print(\"It's a draw!\")\n                break\n            state = state.copy()\n        if render: print()\n    print(f\"Evaluation Results: Wins: {wins}, Losses: {losses}, Draws: {draws}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.690068Z","iopub.execute_input":"2025-02-14T21:38:00.690399Z","iopub.status.idle":"2025-02-14T21:38:00.708118Z","shell.execute_reply.started":"2025-02-14T21:38:00.690373Z","shell.execute_reply":"2025-02-14T21:38:00.707152Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 5000","metadata":{}},{"cell_type":"code","source":"env = TicTacToe()\nagent = QLearningAgent(env)\ntrain(agent, 5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:00.709554Z","iopub.execute_input":"2025-02-14T21:38:00.709907Z","iopub.status.idle":"2025-02-14T21:38:02.220049Z","shell.execute_reply.started":"2025-02-14T21:38:00.709870Z","shell.execute_reply":"2025-02-14T21:38:02.219039Z"}},"outputs":[{"name":"stdout","text":"Episode 500/5000 completed.\nEpisode 1000/5000 completed.\nEpisode 1500/5000 completed.\nEpisode 2000/5000 completed.\nEpisode 2500/5000 completed.\nEpisode 3000/5000 completed.\nEpisode 3500/5000 completed.\nEpisode 4000/5000 completed.\nEpisode 4500/5000 completed.\nEpisode 5000/5000 completed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# import random\n# for _ in range(3):\n#     random_key = random.choice(list(agent.q_table.keys()))\n#     print(random_key)\n#     print(agent.q_table[random_key])\n#     print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:02.221216Z","iopub.execute_input":"2025-02-14T21:38:02.221507Z","iopub.status.idle":"2025-02-14T21:38:02.225357Z","shell.execute_reply.started":"2025-02-14T21:38:02.221475Z","shell.execute_reply":"2025-02-14T21:38:02.224303Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"evaluate(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:02.226342Z","iopub.execute_input":"2025-02-14T21:38:02.226676Z","iopub.status.idle":"2025-02-14T21:38:02.277789Z","shell.execute_reply.started":"2025-02-14T21:38:02.226641Z","shell.execute_reply":"2025-02-14T21:38:02.276875Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 62, Losses: 25, Draws: 13\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 10000","metadata":{}},{"cell_type":"code","source":"env = TicTacToe()\nagent = QLearningAgent(env)\ntrain(agent, 10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:02.279823Z","iopub.execute_input":"2025-02-14T21:38:02.280201Z","iopub.status.idle":"2025-02-14T21:38:05.183928Z","shell.execute_reply.started":"2025-02-14T21:38:02.280172Z","shell.execute_reply":"2025-02-14T21:38:05.182897Z"}},"outputs":[{"name":"stdout","text":"Episode 500/10000 completed.\nEpisode 1000/10000 completed.\nEpisode 1500/10000 completed.\nEpisode 2000/10000 completed.\nEpisode 2500/10000 completed.\nEpisode 3000/10000 completed.\nEpisode 3500/10000 completed.\nEpisode 4000/10000 completed.\nEpisode 4500/10000 completed.\nEpisode 5000/10000 completed.\nEpisode 5500/10000 completed.\nEpisode 6000/10000 completed.\nEpisode 6500/10000 completed.\nEpisode 7000/10000 completed.\nEpisode 7500/10000 completed.\nEpisode 8000/10000 completed.\nEpisode 8500/10000 completed.\nEpisode 9000/10000 completed.\nEpisode 9500/10000 completed.\nEpisode 10000/10000 completed.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"evaluate(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:05.185111Z","iopub.execute_input":"2025-02-14T21:38:05.185374Z","iopub.status.idle":"2025-02-14T21:38:05.225950Z","shell.execute_reply.started":"2025-02-14T21:38:05.185352Z","shell.execute_reply":"2025-02-14T21:38:05.224667Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 63, Losses: 20, Draws: 17\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 15000","metadata":{}},{"cell_type":"code","source":"env = TicTacToe()\nagent = QLearningAgent(env)\ntrain(agent, 15000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:05.227049Z","iopub.execute_input":"2025-02-14T21:38:05.227357Z","iopub.status.idle":"2025-02-14T21:38:09.731136Z","shell.execute_reply.started":"2025-02-14T21:38:05.227324Z","shell.execute_reply":"2025-02-14T21:38:09.730063Z"}},"outputs":[{"name":"stdout","text":"Episode 500/15000 completed.\nEpisode 1000/15000 completed.\nEpisode 1500/15000 completed.\nEpisode 2000/15000 completed.\nEpisode 2500/15000 completed.\nEpisode 3000/15000 completed.\nEpisode 3500/15000 completed.\nEpisode 4000/15000 completed.\nEpisode 4500/15000 completed.\nEpisode 5000/15000 completed.\nEpisode 5500/15000 completed.\nEpisode 6000/15000 completed.\nEpisode 6500/15000 completed.\nEpisode 7000/15000 completed.\nEpisode 7500/15000 completed.\nEpisode 8000/15000 completed.\nEpisode 8500/15000 completed.\nEpisode 9000/15000 completed.\nEpisode 9500/15000 completed.\nEpisode 10000/15000 completed.\nEpisode 10500/15000 completed.\nEpisode 11000/15000 completed.\nEpisode 11500/15000 completed.\nEpisode 12000/15000 completed.\nEpisode 12500/15000 completed.\nEpisode 13000/15000 completed.\nEpisode 13500/15000 completed.\nEpisode 14000/15000 completed.\nEpisode 14500/15000 completed.\nEpisode 15000/15000 completed.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"evaluate(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:38:09.732310Z","iopub.execute_input":"2025-02-14T21:38:09.732662Z","iopub.status.idle":"2025-02-14T21:38:09.786088Z","shell.execute_reply.started":"2025-02-14T21:38:09.732633Z","shell.execute_reply":"2025-02-14T21:38:09.784868Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results: Wins: 64, Losses: 22, Draws: 14\n","output_type":"stream"}],"execution_count":12}]}