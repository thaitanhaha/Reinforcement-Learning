{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:34.934756Z","iopub.execute_input":"2025-02-06T18:24:34.935220Z","iopub.status.idle":"2025-02-06T18:24:34.940628Z","shell.execute_reply.started":"2025-02-06T18:24:34.935178Z","shell.execute_reply":"2025-02-06T18:24:34.939437Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class TicTacToe:\n    def __init__(self):\n        self.board = [0] * 9\n        self.current_player = 1\n    \n    def reset(self):\n        self.board = [0] * 9\n        self.current_player = 1\n        return self.board\n    \n    def get_available_actions(self):\n        return [i for i, x in enumerate(self.board) if x == 0]\n    \n    def make_move(self, action):\n        self.board[action] = self.current_player\n        if self.check_win(self.current_player):\n            return self.current_player\n        elif self.is_full():\n            return 0\n        else:\n            self.current_player *= -1\n            return None\n    \n    def check_win(self, player):\n        win_conditions = [\n            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n            [0, 4, 8], [2, 4, 6]\n        ]\n        for condition in win_conditions:\n            if all(self.board[i] == player for i in condition):\n                return True\n        return False\n    \n    def is_full(self):\n        return all(x != 0 for x in self.board)\n\n    def render(self):\n        for i in range(0, 9, 3):\n            print(self.board[i:i+3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:34.942185Z","iopub.execute_input":"2025-02-06T18:24:34.942609Z","iopub.status.idle":"2025-02-06T18:24:34.965267Z","shell.execute_reply.started":"2025-02-06T18:24:34.942554Z","shell.execute_reply":"2025-02-06T18:24:34.964194Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class QLearningAgent:\n    def __init__(self, environment, alpha=0.1, gamma=0.9, epsilon=0.1):\n        self.env = environment\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.q_table = {}\n        \n    def get_state(self, board):\n        return tuple(self.env.board)\n    \n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.choice(self.env.get_available_actions())\n        else:\n            if state not in self.q_table:\n                self.q_table[state] = np.zeros(9)\n            available_actions = self.env.get_available_actions()\n            q_values = [self.q_table[state][i] if i in available_actions else -np.inf for i in range(9)]\n            max_q_value = np.max(q_values)\n            max_actions = [i for i in range(9) if q_values[i] == max_q_value]\n            chosen_action = np.random.choice(max_actions)\n            return available_actions[available_actions.index(chosen_action)]\n    \n    def update_q_table(self, state, action, reward, next_state):\n        if next_state not in self.q_table:\n            self.q_table[next_state] = np.zeros(9)\n        self.q_table[state][action] += self.alpha * (\n            reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state][action]\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:35.012710Z","iopub.execute_input":"2025-02-06T18:24:35.013093Z","iopub.status.idle":"2025-02-06T18:24:35.021908Z","shell.execute_reply.started":"2025-02-06T18:24:35.013064Z","shell.execute_reply":"2025-02-06T18:24:35.020766Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def train(agent, num_episodes=10000):\n    for episode in range(num_episodes):\n        state = agent.env.reset()\n        done = False\n        while not done:\n            action = agent.choose_action(agent.get_state(state))\n            reward = agent.env.make_move(action)\n            next_state = state.copy()\n            if reward is not None:\n                done = True\n                agent.update_q_table(agent.get_state(state), action, reward, agent.get_state(next_state))\n            state = next_state\n    \n        if (episode + 1) % 1000 == 0:\n            print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n\nenv = TicTacToe()\nagent = QLearningAgent(env)\ntrain(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:35.044606Z","iopub.execute_input":"2025-02-06T18:24:35.044982Z","iopub.status.idle":"2025-02-06T18:24:38.392403Z","shell.execute_reply.started":"2025-02-06T18:24:35.044954Z","shell.execute_reply":"2025-02-06T18:24:38.391261Z"}},"outputs":[{"name":"stdout","text":"Episode 1000/10000 completed.\nEpisode 2000/10000 completed.\nEpisode 3000/10000 completed.\nEpisode 4000/10000 completed.\nEpisode 5000/10000 completed.\nEpisode 6000/10000 completed.\nEpisode 7000/10000 completed.\nEpisode 8000/10000 completed.\nEpisode 9000/10000 completed.\nEpisode 10000/10000 completed.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import random\nfor _ in range(3):\n    random_key = random.choice(list(agent.q_table.keys()))\n    print(random_key)\n    print(agent.q_table[random_key])\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:38.394103Z","iopub.execute_input":"2025-02-06T18:24:38.394579Z","iopub.status.idle":"2025-02-06T18:24:38.404645Z","shell.execute_reply.started":"2025-02-06T18:24:38.394535Z","shell.execute_reply":"2025-02-06T18:24:38.403454Z"}},"outputs":[{"name":"stdout","text":"(0, 0, 1, 0, -1, 1, 1, -1, -1)\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n(1, -1, 0, -1, -1, 1, -1, 1, 1)\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n(0, 1, 1, 0, -1, 0, -1, 1, 0)\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def test(agent, num_episodes=10): \n    for _ in range(num_episodes):\n        state = agent.env.reset()\n        done = False\n        while not done:\n            agent.env.render()\n            action = agent.choose_action(agent.get_state(state))\n            print(f\"Player {1 if agent.env.current_player == 1 else -1} chooses position {action}\")\n            reward = agent.env.make_move(action)\n            if reward is not None:\n                agent.env.render()\n                if reward == 1:\n                    print(\"WIN!\")\n                elif reward == -1:\n                    print(\"LOSE!\")\n                elif reward == 0:\n                    print(\"It's a draw!\")\n                break\n            state = state.copy()\n        print()\n\ntest(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:24:38.406477Z","iopub.execute_input":"2025-02-06T18:24:38.406934Z","iopub.status.idle":"2025-02-06T18:24:38.525947Z","shell.execute_reply.started":"2025-02-06T18:24:38.406887Z","shell.execute_reply":"2025-02-06T18:24:38.524765Z"}},"outputs":[{"name":"stdout","text":"[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 0\n[1, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer -1 chooses position 8\n[1, 0, 0]\n[0, 0, 0]\n[0, 0, -1]\nPlayer 1 chooses position 3\n[1, 0, 0]\n[1, 0, 0]\n[0, 0, -1]\nPlayer -1 chooses position 2\n[1, 0, -1]\n[1, 0, 0]\n[0, 0, -1]\nPlayer 1 chooses position 5\n[1, 0, -1]\n[1, 0, 1]\n[0, 0, -1]\nPlayer -1 chooses position 7\n[1, 0, -1]\n[1, 0, 1]\n[0, -1, -1]\nPlayer 1 chooses position 4\n[1, 0, -1]\n[1, 1, 1]\n[0, -1, -1]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 4\n[0, 0, 0]\n[0, 1, 0]\n[0, 0, 0]\nPlayer -1 chooses position 0\n[-1, 0, 0]\n[0, 1, 0]\n[0, 0, 0]\nPlayer 1 chooses position 6\n[-1, 0, 0]\n[0, 1, 0]\n[1, 0, 0]\nPlayer -1 chooses position 8\n[-1, 0, 0]\n[0, 1, 0]\n[1, 0, -1]\nPlayer 1 chooses position 2\n[-1, 0, 1]\n[0, 1, 0]\n[1, 0, -1]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 7\n[0, 0, 0]\n[0, 0, 0]\n[0, 1, 0]\nPlayer -1 chooses position 2\n[0, 0, -1]\n[0, 0, 0]\n[0, 1, 0]\nPlayer 1 chooses position 1\n[0, 1, -1]\n[0, 0, 0]\n[0, 1, 0]\nPlayer -1 chooses position 0\n[-1, 1, -1]\n[0, 0, 0]\n[0, 1, 0]\nPlayer 1 chooses position 6\n[-1, 1, -1]\n[0, 0, 0]\n[1, 1, 0]\nPlayer -1 chooses position 5\n[-1, 1, -1]\n[0, 0, -1]\n[1, 1, 0]\nPlayer 1 chooses position 4\n[-1, 1, -1]\n[0, 1, -1]\n[1, 1, 0]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 6\n[0, 0, 0]\n[0, 0, 0]\n[1, 0, 0]\nPlayer -1 chooses position 5\n[0, 0, 0]\n[0, 0, -1]\n[1, 0, 0]\nPlayer 1 chooses position 4\n[0, 0, 0]\n[0, 1, -1]\n[1, 0, 0]\nPlayer -1 chooses position 0\n[-1, 0, 0]\n[0, 1, -1]\n[1, 0, 0]\nPlayer 1 chooses position 2\n[-1, 0, 1]\n[0, 1, -1]\n[1, 0, 0]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 4\n[0, 0, 0]\n[0, 1, 0]\n[0, 0, 0]\nPlayer -1 chooses position 3\n[0, 0, 0]\n[-1, 1, 0]\n[0, 0, 0]\nPlayer 1 chooses position 0\n[1, 0, 0]\n[-1, 1, 0]\n[0, 0, 0]\nPlayer -1 chooses position 8\n[1, 0, 0]\n[-1, 1, 0]\n[0, 0, -1]\nPlayer 1 chooses position 2\n[1, 0, 1]\n[-1, 1, 0]\n[0, 0, -1]\nPlayer -1 chooses position 6\n[1, 0, 1]\n[-1, 1, 0]\n[-1, 0, -1]\nPlayer 1 chooses position 1\n[1, 1, 1]\n[-1, 1, 0]\n[-1, 0, -1]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 0\n[1, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer -1 chooses position 7\n[1, 0, 0]\n[0, 0, 0]\n[0, -1, 0]\nPlayer 1 chooses position 2\n[1, 0, 1]\n[0, 0, 0]\n[0, -1, 0]\nPlayer -1 chooses position 3\n[1, 0, 1]\n[-1, 0, 0]\n[0, -1, 0]\nPlayer 1 chooses position 6\n[1, 0, 1]\n[-1, 0, 0]\n[1, -1, 0]\nPlayer -1 chooses position 1\n[1, -1, 1]\n[-1, 0, 0]\n[1, -1, 0]\nPlayer 1 chooses position 8\n[1, -1, 1]\n[-1, 0, 0]\n[1, -1, 1]\nPlayer -1 chooses position 4\n[1, -1, 1]\n[-1, -1, 0]\n[1, -1, 1]\nLOSE!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 6\n[0, 0, 0]\n[0, 0, 0]\n[1, 0, 0]\nPlayer -1 chooses position 1\n[0, -1, 0]\n[0, 0, 0]\n[1, 0, 0]\nPlayer 1 chooses position 5\n[0, -1, 0]\n[0, 0, 1]\n[1, 0, 0]\nPlayer -1 chooses position 2\n[0, -1, -1]\n[0, 0, 1]\n[1, 0, 0]\nPlayer 1 chooses position 0\n[1, -1, -1]\n[0, 0, 1]\n[1, 0, 0]\nPlayer -1 chooses position 8\n[1, -1, -1]\n[0, 0, 1]\n[1, 0, -1]\nPlayer 1 chooses position 4\n[1, -1, -1]\n[0, 1, 1]\n[1, 0, -1]\nPlayer -1 chooses position 3\n[1, -1, -1]\n[-1, 1, 1]\n[1, 0, -1]\nPlayer 1 chooses position 7\n[1, -1, -1]\n[-1, 1, 1]\n[1, 1, -1]\nIt's a draw!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 8\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 1]\nPlayer -1 chooses position 4\n[0, 0, 0]\n[0, -1, 0]\n[0, 0, 1]\nPlayer 1 chooses position 5\n[0, 0, 0]\n[0, -1, 1]\n[0, 0, 1]\nPlayer -1 chooses position 3\n[0, 0, 0]\n[-1, -1, 1]\n[0, 0, 1]\nPlayer 1 chooses position 1\n[0, 1, 0]\n[-1, -1, 1]\n[0, 0, 1]\nPlayer -1 chooses position 6\n[0, 1, 0]\n[-1, -1, 1]\n[-1, 0, 1]\nPlayer 1 chooses position 2\n[0, 1, 1]\n[-1, -1, 1]\n[-1, 0, 1]\nWIN!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 0\n[1, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer -1 chooses position 2\n[1, 0, -1]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 6\n[1, 0, -1]\n[0, 0, 0]\n[1, 0, 0]\nPlayer -1 chooses position 5\n[1, 0, -1]\n[0, 0, -1]\n[1, 0, 0]\nPlayer 1 chooses position 4\n[1, 0, -1]\n[0, 1, -1]\n[1, 0, 0]\nPlayer -1 chooses position 8\n[1, 0, -1]\n[0, 1, -1]\n[1, 0, -1]\nLOSE!\n\n[0, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 0\n[1, 0, 0]\n[0, 0, 0]\n[0, 0, 0]\nPlayer -1 chooses position 2\n[1, 0, -1]\n[0, 0, 0]\n[0, 0, 0]\nPlayer 1 chooses position 3\n[1, 0, -1]\n[1, 0, 0]\n[0, 0, 0]\nPlayer -1 chooses position 4\n[1, 0, -1]\n[1, -1, 0]\n[0, 0, 0]\nPlayer 1 chooses position 5\n[1, 0, -1]\n[1, -1, 1]\n[0, 0, 0]\nPlayer -1 chooses position 1\n[1, -1, -1]\n[1, -1, 1]\n[0, 0, 0]\nPlayer 1 chooses position 8\n[1, -1, -1]\n[1, -1, 1]\n[0, 0, 1]\nPlayer -1 chooses position 7\n[1, -1, -1]\n[1, -1, 1]\n[0, -1, 1]\nLOSE!\n\n","output_type":"stream"}],"execution_count":6}]}